from ast import List
from typing import Any
import dspy
from dspy import GEPA
from dspy.teleprompt import MIPROv2
import pandas as pd
from pydantic import BaseModel, Field, TypeAdapter
from typing_extensions import TypedDict
import pathlib
import logging
from dspy.utils.logging_utils import DSPyLoggingStream
import sys
import os
from dotenv import load_dotenv
from llama_stack_client import LlamaStackClient
import random

load_dotenv()
LLAMA_STACK_URL = os.getenv("LLAMA_STACK_URL")

REASON_LLM_KEY = os.getenv("REASON_LLM_KEY")
REASON_LLM_HOST = os.getenv("REASON_LLM_HOST")
REASON_LLM_MODEL = os.getenv("REASON_LLM_MODEL")


root = logging.getLogger()
root.setLevel(logging.WARN)
handler = logging.StreamHandler(sys.stdout)
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
handler.setFormatter(formatter)
root.addHandler(handler)


class AILanguageDetector(dspy.Signature):
    """Determine if the input_text is generated by AI or by a human"""

    input_text: str = dspy.InputField(desc=("text to be evaluated"))
    result: int = dspy.OutputField(desc=("0 for human 1 for AI"))


def score_pred(gold, pred):
    """
    Compute score.
    """

    score = 1.0 if gold == pred.result else 0.0
    return score


def mipro_metric(example, pred, trace=None, pred_name=None, pred_trace=None):
    # Parse gold standard from example
    gold = example["result"]

    # Compute scores for all modules
    score_val = score_pred(gold, pred)
    return score_val


def gepa_metric(example, pred, trace=None, pred_name=None, pred_trace=None):
    """
    Generate feedback for the urgency module.
    """
    gold = example["result"]

    # Compute scores for all modules
    score_val = score_pred(gold, pred)

    if score_val == 1:
        feedback = f"You correctly determined that the message was generated by `{gold}`. This message is indeed generated by `{gold}`."
    else:
        feedback = f"You incorrectly determined that the message was generated by `{pred.result}`. The correct urgency is `{gold}`. Think about how you could have reasoned to determine the correct source of the message."
    return feedback, score_val


def optimise_prompt_gepa():
    text_processor = dspy.Predict(AILanguageDetector)

    optimizer = dspy.GEPA(
        metric=gepa_metric,
        auto="light",
        num_threads=1,
        track_stats=True,
        use_merge=False,
        reflection_lm=reflection_lm,
    )

    optimized_program = optimizer.compile(
        text_processor,
        trainset=train_set,
        valset=val_set,
    )

    evaluate_prompt(optimized_program, val_set)
    optimized_program.save("gepa-prompt.json")


def optimise_prompt_bootstrap():
    text_processor = dspy.Predict(AILanguageDetector)

    optimizer = dspy.BootstrapFinetune(num_threads=16)
    optimized_program = optimizer.compile(
        text_processor,
        trainset=train_set,
    )

    optimized_program.save("bootstrap-prompt.json")
    evaluate_prompt(optimized_program, val_set)


def optimise_prompt_miprov2():
    text_processor = dspy.Predict(AILanguageDetector)
    print(text_processor.dump_state())
    optimizer = MIPROv2(
        metric=mipro_metric,
        auto="light",
        verbose=True
    )
    optimized_program = optimizer.compile(
        text_processor,
        trainset=train_set,
        valset=val_set
    )
    print(optimized_program.dump_state())
    
    optimized_program.save("mipro-prompt.json")

    evaluate_prompt(optimized_program, val_set)



def evaluate_prompt(text_processor: Any, validation_set: List):
    evaluate = dspy.Evaluate(
        devset=validation_set,
        metric=mipro_metric,
        num_threads=5,
        display_table=10,
        display_progress=True,
        provide_traceback=True,
    )
    evaluate(text_processor)


def init_dataset():
    df = pd.read_csv("data/balanced_ai_human_prompts.csv")
    full_dspy_dataset=[]
    for index, row in df.iterrows():
        full_dspy_dataset.append(
            dspy.Example(
                {
                    "input_text": row["text"],
                    "result": row["generated"],
                }
            ).with_inputs("input_text")
        )

    random.Random(0).shuffle(full_dspy_dataset)
    train_set = full_dspy_dataset[: int(len(full_dspy_dataset) * 0.33)]
    val_set = full_dspy_dataset[
        int(len(full_dspy_dataset) * 0.33) : int(len(full_dspy_dataset) * 0.66)
    ]
    test_set = full_dspy_dataset[int(len(full_dspy_dataset) * 0.66) :]

    return full_dspy_dataset, train_set, val_set, test_set


if __name__ == "__main__":
    full_dspy_dataset, train_set, val_set, test_set = init_dataset()
    print(
        f"{len(full_dspy_dataset)}---Training:{len(train_set)}, Validation:{len(val_set)}, Test:{len(test_set)}"
    )

    lls_client = LlamaStackClient(base_url=LLAMA_STACK_URL)
    model_list = lls_client.models.list()
    llm = dspy.LM(
        "openai/" + model_list[0].identifier,
        api_base=LLAMA_STACK_URL + "/v1/openai/v1",
        model_type="chat",
        api_key="this is a fake key",
    )

    reflection_lm = dspy.LM(
        model=REASON_LLM_MODEL, api_base=REASON_LLM_HOST, api_key=REASON_LLM_KEY
    )

    LOGGING_LINE_FORMAT = "%(asctime)s %(levelname)s %(name)s: %(message)s"
    LOGGING_DATETIME_FORMAT = "%Y/%m/%d %H:%M:%S"
    dspy.configure(lm=llm)
    dspy.configure_cache(
        enable_disk_cache=False,
        enable_memory_cache=False,
    )
    # static_run()
    # evaluate_prompt()
    # optimise_prompt_gepa()
    # optimise_prompt_bootstrap()
    # optimise_prompt_miprov2()

    text_processor = dspy.Predict(AILanguageDetector)
    
    text_processor.load("mipro-prompt-orig.json")
    print(text_processor.dump_state())
    evaluate_prompt(text_processor,full_dspy_dataset)
