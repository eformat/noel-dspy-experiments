from ast import List
from typing import Any
import dspy
from dspy import GEPA
from dspy.teleprompt import MIPROv2
import pandas as pd
from pydantic import BaseModel, Field, TypeAdapter
from typing_extensions import TypedDict
import pathlib
import logging
from dspy.utils.logging_utils import DSPyLoggingStream
import sys
import os
from dotenv import load_dotenv
from llama_stack_client import LlamaStackClient
import random
from rich import print

load_dotenv()
LLAMA_STACK_URL = os.getenv("LLAMA_STACK_URL")

#REASON_LLM_KEY = os.getenv("REASON_LLM_KEY")
#REASON_LLM_HOST = os.getenv("REASON_LLM_HOST")
#REASON_LLM_MODEL = os.getenv("REASON_LLM_MODEL")


root = logging.getLogger()
root.setLevel(logging.WARN)
handler = logging.StreamHandler(sys.stdout)
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
handler.setFormatter(formatter)
root.addHandler(handler)


class AILanguageDetector(dspy.Signature):
    """Determine if the input_text is generated by AI or by a human"""

    input_text: str = dspy.InputField(desc=("text to be evaluated"))
    result: int = dspy.OutputField(desc=("0 for human 1 for AI"))


def score_pred(gold, pred):
    """
    Compute score.
    """

    score = 1.0 if gold == pred.result else 0.0
    return score


def mipro_metric(example, pred, trace=None, pred_name=None, pred_trace=None):
    # Parse gold standard from example
    gold = example["result"]

    # Compute scores for all modules
    score_val = score_pred(gold, pred)
    return score_val


def gepa_metric(example, pred, trace=None, pred_name=None, pred_trace=None):
    """
    Generate feedback for the urgency module.
    """
    gold = example["result"]

    # Compute scores for all modules
    score_val = score_pred(gold, pred)

    if score_val == 1:
        feedback = f"You correctly determined that the message was generated by `{gold}`. This message is indeed generated by `{gold}`."
    else:
        feedback = f"You incorrectly determined that the message was generated by `{pred.result}`. The correct urgency is `{gold}`. Think about how you could have reasoned to determine the correct source of the message."
    return feedback, score_val


def optimise_prompt_gepa():
    text_processor = dspy.Predict(AILanguageDetector)

    optimizer = dspy.GEPA(
        metric=gepa_metric,
        auto="light",
        num_threads=1,
        track_stats=True,
        use_merge=False,
        reflection_lm=reflection_lm,
    )

    optimized_program = optimizer.compile(
        text_processor,
        trainset=train_set,
        valset=val_set,
    )

    evaluate_prompt(optimized_program, val_set)
    optimized_program.save("gepa-prompt.json")


def optimise_prompt_bootstrap():
    text_processor = dspy.Predict(AILanguageDetector)

    optimizer = dspy.BootstrapFinetune(num_threads=16)
    optimized_program = optimizer.compile(
        text_processor,
        trainset=train_set,
    )

    optimized_program.save("bootstrap-prompt.json")
    evaluate_prompt(optimized_program, val_set)


def optimise_prompt_miprov2():
    text_processor = dspy.Predict(AILanguageDetector)
    print(text_processor.dump_state())
    optimizer = MIPROv2(
        metric=mipro_metric,
        auto="light",
        verbose=True
    )
    optimized_program = optimizer.compile(
        text_processor,
        trainset=train_set,
        valset=val_set
    )
    print(optimized_program.dump_state())
    
    optimized_program.save("mipro-prompt.json")

    evaluate_prompt(optimized_program, val_set)



def evaluate_prompt(text_processor: Any, validation_set: List):
    evaluate = dspy.Evaluate(
        devset=validation_set,
        metric=mipro_metric,
        num_threads=5,
        display_table=10,
        display_progress=True,
        provide_traceback=True,
    )
    evaluate(text_processor)


def init_dataset():
    df = pd.read_csv("data/balanced_ai_human_prompts.csv")
    full_dspy_dataset=[]
    for index, row in df.iterrows():
        full_dspy_dataset.append(
            dspy.Example(
                {
                    "input_text": row["text"],
                    "result": row["generated"],
                }
            ).with_inputs("input_text")
        )

    random.Random(0).shuffle(full_dspy_dataset)
    train_set = full_dspy_dataset[: int(len(full_dspy_dataset) * 0.33)]
    val_set = full_dspy_dataset[
        int(len(full_dspy_dataset) * 0.33) : int(len(full_dspy_dataset) * 0.66)
    ]
    test_set = full_dspy_dataset[int(len(full_dspy_dataset) * 0.66) :]

    return full_dspy_dataset, train_set, val_set, test_set


if __name__ == "__main__":
    full_dspy_dataset, train_set, val_set, test_set = init_dataset()
    print(
        f"{len(full_dspy_dataset)}---Training:{len(train_set)}, Validation:{len(val_set)}, Test:{len(test_set)}"
    )

    lls_client = LlamaStackClient(base_url=LLAMA_STACK_URL)
    model_list = lls_client.models.list()
    print(model_list)
    llm = dspy.LM(
        "openai/" + model_list[2].identifier,
        api_base=LLAMA_STACK_URL + "/v1/openai/v1",
        model_type="chat",
        api_key="this is a fake key",
    )

    # reflection_lm = dspy.LM(
    #     model=REASON_LLM_MODEL, api_base=REASON_LLM_HOST, api_key=REASON_LLM_KEY
    # )

    LOGGING_LINE_FORMAT = "%(asctime)s %(levelname)s %(name)s: %(message)s"
    LOGGING_DATETIME_FORMAT = "%Y/%m/%d %H:%M:%S"
    dspy.configure(lm=llm)
    dspy.configure_cache(
        enable_disk_cache=False,
        enable_memory_cache=False,
    )
    # static_run()
    # evaluate_prompt()
    # optimise_prompt_gepa()
    # optimise_prompt_bootstrap()
    optimise_prompt_miprov2()
    # print(human_count)
    # dspy.inspect_history(n=10)
    text_processor = dspy.Predict(AILanguageDetector)
    # text_processor.save("base-prompt.json")
    #text_processor.load("base-prompt.json")
    text_processor.load("mipro-prompt.json")
    # evaluate_prompt(text_processor)
    print(text_processor.dump_state())
    evaluate_prompt(text_processor,full_dspy_dataset)


# On validation data set (908 examples)
#2025/09/12 14:03:11 INFO dspy.evaluate.evaluate: Average Metric: 884.0 / 908 (97.4%) - Miprov2
# 2025/09/12 14:33:14 INFO dspy.evaluate.evaluate: Average Metric: 512.0 / 908 (56.4%) - Base prompt

# vllm-llama-4-guard/llama-4-scout-17b-16e-w4a16
# text_processor.load("base-prompt.json")
#Average Metric: 1456.00 / 2750 (52.9%): 100%|████████████████████████████| 2750/2750 [12:08<00:00,  3.78it/s]
#2025/09/14 10:10:47 INFO dspy.evaluate.evaluate: Average Metric: 1456.0 / 2750 (52.9%)

# text_processor.load("mipro-prompt.json")
#Average Metric: 2717.00 / 2750 (98.8%): 100%|████████████████████████████| 2750/2750 [11:01<00:00,  4.16it/s]
#2025/09/14 10:33:00 INFO dspy.evaluate.evaluate: Average Metric: 2717.0 / 2750 (98.8%)
    # optimise_prompt_miprov2()
