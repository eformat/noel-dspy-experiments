from typing import Any
import dspy
from dspy import GEPA
from dspy.teleprompt import MIPROv2
import pandas as pd
from pydantic import BaseModel, Field, TypeAdapter
from typing_extensions import TypedDict
import pathlib
import logging
from dspy.utils.logging_utils import DSPyLoggingStream
import sys
import os
from dotenv import load_dotenv
from llama_stack_client import LlamaStackClient
import random

load_dotenv()
LLAMA_STACK_URL = os.getenv("LLAMA_STACK_URL")

REASON_LLM_KEY = os.getenv("REASON_LLM_KEY")
REASON_LLM_HOST = os.getenv("REASON_LLM_HOST")
REASON_LLM_MODEL = os.getenv("REASON_LLM_MODEL")


root = logging.getLogger()
root.setLevel(logging.WARN)
handler = logging.StreamHandler(sys.stdout)
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
handler.setFormatter(formatter)
root.addHandler(handler)


class AILanguageDetector(dspy.Signature):
    """Determine if the input_text is generated by AI or by a human"""

    input_text: str = dspy.InputField(desc=("text to be evaluated"))
    result: int = dspy.OutputField(desc=("0 for human 1 for AI"))


# def static_run():
#     texts = df['text_content'].tolist()
#     print(texts[0])

#     text_processor = dspy.Predict(AILanguageDetector)

#     result = text_processor(input_text=texts[0])
#     print(result.result)
#     dspy.inspect_history(n=50)


def score_pred(gold, pred):
    """
    Compute score.
    """
    # if pred.result == 0:
    #     print("HUMAN")
    #     human_count+1

    score = 1.0 if gold == pred.result else 0.0
    # print(f'Score {score} Gold {gold}, Prediction {pred.result}')
    return score


def metric(example, pred, trace=None, pred_name=None, pred_trace=None):
    # Parse gold standard from example
    gold = example["result"]

    # Compute scores for all modules
    score_val = score_pred(gold, pred)
    return score_val


def gepa_metric(example, pred, trace=None, pred_name=None, pred_trace=None):
    """
    Generate feedback for the urgency module.
    """
    gold = example["result"]

    # Compute scores for all modules
    score_val = score_pred(gold, pred)

    if score_val == 1:
        feedback = f"You correctly determined that the message was generated by `{gold}`. This message is indeed generated by `{gold}`."
    else:
        feedback = f"You incorrectly determined that the message was generated by `{pred.result}`. The correct urgency is `{gold}`. Think about how you could have reasoned to determine the correct source of the message."
    return feedback, score_val


def optimise_prompt_gepa():
    text_processor = dspy.Predict(AILanguageDetector)

    optimizer = dspy.GEPA(
        metric=gepa_metric,
        auto="light", 
        num_threads=1,
        track_stats=True,
        use_merge=False,
        reflection_lm=reflection_lm
    )
    
    optimized_program = optimizer.compile(
        text_processor,
        trainset=train_set,
        valset=val_set,
    )

    evaluate_prompt(optimized_program)
    optimized_program.save("gepa-prompt.json")    

def optimise_prompt_bootstrap():
    text_processor = dspy.Predict(AILanguageDetector)
    
    optimizer = dspy.BootstrapFinetune(num_threads=16)
    optimized_program = optimizer.compile(
        text_processor,
        trainset=train_set,
        # valset=val_set,
    )

    evaluate_prompt(optimized_program)
    optimized_program.save("bootstrap-prompt.json")
    
def optimise_prompt_miprov2():
    text_processor = dspy.Predict(AILanguageDetector)
    
    optimizer = MIPROv2(
        metric=metric,
        auto="medium", # Can choose between light, medium, and heavy optimization runs
    )
    optimized_program = optimizer.compile(
        text_processor,
        trainset=train_set,
    )

    evaluate_prompt(optimized_program)

    optimized_program.save("mipro-prompt.json")
 

def evaluate_prompt(text_processor:Any):

    evaluate = dspy.Evaluate(
        devset=dspy_dataset,
        metric=metric,
        num_threads=5,
        display_table=10,
        display_progress=True,
        provide_traceback=True,
    )
    evaluate(text_processor)


def init_dataset():
    df = pd.read_csv("data/balanced_ai_human_prompts.csv")
    print(df)

    for index, row in df.iterrows():
        dspy_dataset.append(
            dspy.Example(
                {
                    "input_text": row["text"],
                    "result": row["generated"],
                }
            ).with_inputs("input_text")
        )

    random.Random(0).shuffle(dspy_dataset)
    train_set = dspy_dataset[: int(len(dspy_dataset) * 0.33)]
    val_set = dspy_dataset[
        int(len(dspy_dataset) * 0.33) : int(len(dspy_dataset) * 0.66)
    ]
    test_set = dspy_dataset[int(len(dspy_dataset) * 0.66) :]
    print(train_set[:2])

    return train_set, val_set, test_set


if __name__ == "__main__":
    dspy_dataset = []
    human_count = 0
    train_set, val_set, test_set = init_dataset()

    print(f"{len(dspy_dataset)}---{len(train_set)}, {len(val_set)}, {len(test_set)}")

    lls_client = LlamaStackClient(base_url=LLAMA_STACK_URL)
    model_list = lls_client.models.list()
    llm = dspy.LM(
        "openai/" + model_list[0].identifier,
        api_base=LLAMA_STACK_URL + "/v1/openai/v1",
        model_type="chat",
        api_key="this is a fake key",
    )

    reflection_lm = dspy.LM(
        model=REASON_LLM_MODEL, api_base=REASON_LLM_HOST, api_key=REASON_LLM_KEY
    )

    LOGGING_LINE_FORMAT = "%(asctime)s %(levelname)s %(name)s: %(message)s"
    LOGGING_DATETIME_FORMAT = "%Y/%m/%d %H:%M:%S"
    dspy.configure(lm=llm)
    # static_run()
    # evaluate_prompt()
    # optimise_prompt_gepa()
    # optimise_prompt_bootstrap()
    # optimise_prompt_miprov2()
    # print(human_count)
    # dspy.inspect_history(n=10)
    text_processor = dspy.Predict(AILanguageDetector)
    # text_processor.save("base-prompt.json")
    text_processor.load("base-prompt.json")
    evaluate_prompt(text_processor)


# On validation data set (908 examples)
#2025/09/12 14:03:11 INFO dspy.evaluate.evaluate: Average Metric: 884.0 / 908 (97.4%) - Miprov2
# 2025/09/12 14:33:14 INFO dspy.evaluate.evaluate: Average Metric: 512.0 / 908 (56.4%) - Base prompt